{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project proposes to leverage machine learning (ML) and natural language processing (NLP) techniques to build a text classifier that automatizes the processing and identification of evidence of social impact in research documents. The proposal aims to solve a classification problem in which the model takes a sentence contained in a research document as input and produces as output a binary answer (1=True, 0=False) that states whether the sentence contains or not evidence of social impact, respectively.\n",
    "\n",
    "From all research fields, this project focuses on Medical, Health, and Biological science because the ultimately goal is to understand the social impact of the research projects of the Spanish National Institue of Bioinformatics (INB by its Spanish Acronym), which is an institution that conducts medical and biological investigations.\n",
    "\n",
    "The goal of this notebook is to develop a baseline model against to which compare the performance of the machine learning classifier. The base-line model is a vocabulary-based classifier that looks in sentences for occurrences of words in the vocabulary. Sentences are then classified as containing evidence of social impact if they include words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/Life/jsaldiva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/Life/jsaldiva/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/Life/jsaldiva/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "s_nlp = spacy.load('en')\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import corpus, pos_tag, word_tokenize\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from utils import lemmatize_words, normalize, remove_non_ascii, remove_extra_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train_test_data.csv')\n",
    "validation_data = pd.read_csv('./data/validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, validation_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>widely  featured in the national press and rad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indeed one  of these  projects has been select...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>impact public engagement and education influen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reach worldwide dolly became a scientific icon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the educational tools have been used by 11 000...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  widely  featured in the national press and rad...      1\n",
       "1  indeed one  of these  projects has been select...      1\n",
       "2  impact public engagement and education influen...      1\n",
       "3  reach worldwide dolly became a scientific icon...      1\n",
       "4  the educational tools have been used by 11 000...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate labels from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = data['sentence'], data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fist step in implementing the model is to build a vocabulary of words that are commonly used to indicate social impact of research. Again descriptions of social impact of research available in REF were used to construct the dictionary. In particular, I went through the first 230 summaries of impact published [here](https://impact.ref.ac.uk/casestudies/Results.aspx?Type=I&Tag=5085) and tagged verbs and nouns employed in sentences that contain evidence of social impact of research. The identified verbs and nouns were extracted and collected in the file `data/dictionary.csv`. What I discovered during this process is that sentences with evidence of social impact are formed with combinations of these verbs and nouns. The next function reads the dictionary file and creates a list with all combination of nouns and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(data_dir):\n",
    "    impact_words_file_name = pathlib.Path(data_dir, 'dictionary.csv')\n",
    "    impact_words, i_verbs, i_nouns = [], [], []\n",
    "    with open(str(impact_words_file_name), 'r') as f:\n",
    "        file = csv.DictReader(f, delimiter=',')\n",
    "        for line in file:\n",
    "            if line['pos'] == 'verb':\n",
    "                lemma_words = ' '.join(lemmatize_words(normalize(line['word']), pos=corpus.wordnet.VERB))\n",
    "                i_verbs.append(lemma_words)\n",
    "            if line['pos'] == 'noun':\n",
    "                lemma_words = ' '.join(lemmatize_words(normalize(line['word']), pos=corpus.wordnet.NOUN))\n",
    "                i_nouns.append(lemma_words)\n",
    "    impact_words = [i_verb + ' ' + i_noun for i_verb in i_verbs for i_noun in i_nouns if i_verb != i_noun]\n",
    "    return impact_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = build_dictionary(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some entried of the dictionary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ensure accessibility',\n",
       " 'ensure accreditation',\n",
       " 'ensure agenda',\n",
       " 'ensure aggression',\n",
       " 'ensure audience',\n",
       " 'ensure awareness',\n",
       " 'ensure basis',\n",
       " 'ensure behavior',\n",
       " 'ensure benefit',\n",
       " 'ensure campaign']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find evidence of social impact in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_dependencies(sentence):\n",
    "    sentence_dependencies = defaultdict(list)\n",
    "    for token in sentence:\n",
    "        token_text, token_tag, token_dependency_type, token_dependent_text, token_dependent_tag = \\\n",
    "            token.text, token.tag_, token.dep_, token.head.text, token.head.tag_\n",
    "        # only nouns whose object dependency is a verb are considered\n",
    "        if token_tag[0] == 'N' and token_dependency_type == 'dobj' and token_dependent_tag[0] == 'V':\n",
    "            lemma_dependent = ' '.join(lemmatize_words(token_dependent_text))\n",
    "            lemma_token = ' '.join(lemmatize_words(token_text, pos=corpus.wordnet.NOUN))            \n",
    "            sentence_dependencies[lemma_dependent].append(lemma_token)\n",
    "    return sentence_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_nouns_and_verbs(sentence):\n",
    "    # tag sentence and extract nouns and verbs\n",
    "    tagged_tokens = pos_tag(normalized_sentence)                \n",
    "    lemma_tokens = []        \n",
    "    for tagged_token in tagged_tokens:\n",
    "        token, tag = tagged_token\n",
    "        if tag[0] == 'N':\n",
    "            lemma_token = lemmatize_words(token, pos=corpus.wordnet.NOUN)\n",
    "            lemma_tokens.append(' '.join(lemma_token))\n",
    "        elif tag[0] == 'V':\n",
    "            lemma_token = lemmatize_words(token, pos=corpus.wordnet.VERB)\n",
    "            lemma_tokens.append(' '.join(lemma_token))\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurrences(dictionary, sentence, sentence_dependencies):\n",
    "    occurrences = set()\n",
    "    for entry in dictionary:\n",
    "        entry_tokens = word_tokenize(entry)\n",
    "        reg_verb, reg_noun = entry_tokens[0], ' '.join(entry_tokens[1:])\n",
    "        reg_exp = r'^[\\w\\s]+{verb}\\s[\\w\\s]*{noun}[\\w\\s]*$'.format(verb=reg_verb, noun=reg_noun)\n",
    "        if re.search(reg_exp, sentence):\n",
    "            if sentence_dependencies.get(reg_verb):\n",
    "                if reg_noun in sentence_dependencies[reg_verb]:\n",
    "                    occurrences.add(entry)\n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1006/1006 [32:55<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_labels = []\n",
    "with tqdm(total=len(sentences), file=sys.stdout) as pbar:\n",
    "    for sentence in sentences:\n",
    "        pbar.update(1)\n",
    "        # clean and preprocess sentence\n",
    "        normalized_sentence = normalize(sentence)\n",
    "        clean_sentence = remove_non_ascii(sentence)\n",
    "        clean_sentence = remove_extra_spaces(clean_sentence)\n",
    "        nlp_sentence = s_nlp(' '.join(clean_sentence))\n",
    "        # extract sentence dependencies\n",
    "        sentence_dependencies = extract_sentence_dependencies(nlp_sentence)\n",
    "        # extract nouns and verbs\n",
    "        lemma_tokens = extract_sentence_nouns_and_verbs(normalized_sentence)\n",
    "        lemma_sentence = ' '.join(lemma_tokens)\n",
    "        # iterate over the dictionary entries and find occurrences of \n",
    "        # the sentence nouns and verbs\n",
    "        occurrences = find_occurrences(dictionary, lemma_sentence, sentence_dependencies)\n",
    "        if len(occurrences) > 0:\n",
    "            processed_labels.append(1)\n",
    "        else:\n",
    "            processed_labels.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of items in processed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(processed_labels)==data.shape[0], 'Total items are incorrect!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_results = confusion_matrix(labels, processed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[791,   9],\n",
       "       [143,  63]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balaced Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "print(f'Balaced Accuracy: {round(metrics.balanced_accuracy_score(labels, processed_labels),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.31\n"
     ]
    }
   ],
   "source": [
    "print(f'Recall: {round(metrics.recall_score(labels, processed_labels),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.65\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC-AUC: {round(metrics.roc_auc_score(labels, processed_labels),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {round(metrics.precision_score(labels, processed_labels),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.45\n"
     ]
    }
   ],
   "source": [
    "print(f'F1: {round(metrics.f1_score(labels, processed_labels),2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Impact Env",
   "language": "python",
   "name": "impact-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
